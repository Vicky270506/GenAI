{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "V28",
      "authorship_tag": "ABX9TyPV04tmR0aqeCLdsI51yKrm",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "TPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Vicky270506/GenAI/blob/main/NLP.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qb_HHeGsBSpR"
      },
      "outputs": [],
      "source": [
        "import nltk\n",
        "from nltk.sentiment import SentimentIntensityAnalyzer\n",
        "import gradio as gr\n",
        "\n",
        "# Download required NLTK resources\n",
        "nltk.download('vader_lexicon')\n",
        "\n",
        "# Initialize sentiment analyzer\n",
        "sia = SentimentIntensityAnalyzer()\n",
        "\n",
        "def analyze_sentiment(text):\n",
        "    # Get sentiment scores\n",
        "    scores = sia.polarity_scores(text)\n",
        "\n",
        "    # Determine sentiment label\n",
        "    compound_score = scores['compound']\n",
        "    if compound_score >= 0.05:\n",
        "        sentiment = \"Positive 😊\"\n",
        "    elif compound_score <= -0.05:\n",
        "        sentiment = \"Negative 😠\"\n",
        "    else:\n",
        "        sentiment = \"Neutral 😐\"\n",
        "\n",
        "    # Format results\n",
        "    result = f\"\"\"\n",
        "    Sentiment: {sentiment}\n",
        "    Positive: {scores['pos']:.2f}\n",
        "    Neutral: {scores['neu']:.2f}\n",
        "    Negative: {scores['neg']:.2f}\n",
        "    Compound: {scores['compound']:.2f}\n",
        "    \"\"\"\n",
        "    return result\n",
        "\n",
        "# Create Gradio interface\n",
        "demo = gr.Interface(\n",
        "    fn=analyze_sentiment,\n",
        "    inputs=gr.Textbox(lines=3, placeholder=\"Enter text for sentiment analysis...\"),\n",
        "    outputs=\"text\",\n",
        "    title=\"📊 Sentiment Analyzer\",\n",
        "    description=\"Classify text sentiment using NLTK's VADER analyzer\",\n",
        "    examples=[\n",
        "        [\"I absolutely love this product! Best purchase ever!\"],\n",
        "        [\"This service is terrible and disappointing.\"],\n",
        "        [\"The weather is neither good nor bad today.\"]\n",
        "    ]\n",
        ")\n",
        "\n",
        "# Launch the application\n",
        "if __name__ == \"__main__\":\n",
        "    demo.launch()\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import spacy\n",
        "import gradio as gr\n",
        "\n",
        "# Load the English language model from spaCy\n",
        "nlp = spacy.load(\"en_core_web_sm\")\n",
        "\n",
        "def pos_tagging(text):\n",
        "    # Process the input text using spaCy\n",
        "    doc = nlp(text)\n",
        "\n",
        "    # Create a list to hold the results\n",
        "    results = []\n",
        "\n",
        "    # Extract tokens and their POS tags\n",
        "    for token in doc:\n",
        "        results.append(f\"{token.text} | {token.pos_} | {spacy.explain(token.pos_)}\")\n",
        "\n",
        "    # Join results into a single string for display\n",
        "    return \"\\n\".join(results)\n",
        "\n",
        "# Create Gradio interface\n",
        "demo = gr.Interface(\n",
        "    fn=pos_tagging,\n",
        "    inputs=gr.Textbox(lines=5, placeholder=\"Enter text for POS tagging...\"),\n",
        "    outputs=\"text\",\n",
        "    title=\"📝 Part-Of-Speech Tagging\",\n",
        "    description=\"Enter a sentence to see the part-of-speech tags for each word.\",\n",
        "    examples=[\n",
        "        [\"Apple is planning to buy an Indian startup for $1 billion.\"],\n",
        "        [\"The quick brown fox jumps over the lazy dog.\"],\n",
        "        [\"I love programming in Python!\"]\n",
        "    ]\n",
        ")\n",
        "\n",
        "# Launch the application\n",
        "if __name__ == \"__main__\":\n",
        "    demo.launch()\n"
      ],
      "metadata": {
        "id": "GxzTijvbCD31"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from chatterbot import ChatBot\n",
        "from chatterbot.trainers import ListTrainer\n",
        "import gradio as gr\n",
        "\n",
        "# Create a new chatbot instance\n",
        "chatbot = ChatBot(\n",
        "    \"MeeshoBot\",\n",
        "    storage_adapter='chatterbot.storage.SQLStorageAdapter',\n",
        "    logic_adapters=[\n",
        "        'chatterbot.logic.BestMatch'\n",
        "    ]\n",
        ")\n",
        "\n",
        "# Training data for the chatbot\n",
        "conversation = [\n",
        "    \"Hi\",\n",
        "    \"Hello! How can I assist you today?\",\n",
        "\n",
        "    \"What is the status of my order?\",\n",
        "    \"Please provide your order ID to check the status.\",\n",
        "\n",
        "    \"When will my delivery arrive?\",\n",
        "    \"Deliveries usually arrive within 3-5 business days. Please check your order details for specific dates.\",\n",
        "\n",
        "    \"I have an issue with my payment.\",\n",
        "    \"I'm sorry to hear that. Can you please provide more details about the payment issue?\",\n",
        "\n",
        "    \"Can I change my order?\",\n",
        "    \"You can change your order within 30 minutes of placing it. Please provide your order ID.\",\n",
        "\n",
        "    \"Thank you\",\n",
        "    \"You're welcome! If you have any more questions, feel free to ask.\"\n",
        "]\n",
        "\n",
        "# Train the chatbot\n",
        "trainer = ListTrainer(chatbot)\n",
        "trainer.train(conversation)\n",
        "\n",
        "def get_response(user_input):\n",
        "    # Get response from the chatbot\n",
        "    return str(chatbot.get_response(user_input))\n",
        "\n",
        "# Gradio interface function\n",
        "def chatbot_interface(user_input, language, gender):\n",
        "    response = get_response(user_input)\n",
        "\n",
        "    # Customize response based on language and gender preferences\n",
        "    if language == 'Hindi':\n",
        "        response = f\"यहाँ आपकी मदद के लिए है: {response}\"\n",
        "\n",
        "    if gender == 'Female':\n",
        "        response += \" 😊 (female voice)\"\n",
        "\n",
        "    return response\n",
        "\n",
        "# Create Gradio interface\n",
        "demo = gr.Interface(\n",
        "    fn=chatbot_interface,\n",
        "    inputs=[\n",
        "        gr.Textbox(lines=2, placeholder=\"Ask me anything about orders or payments...\"),\n",
        "        gr.Radio([\"English\", \"Hindi\"], label=\"Select Language\"),\n",
        "        gr.Radio([\"Male\", \"Female\"], label=\"Select Gender\")\n",
        "    ],\n",
        "    outputs=\"text\",\n",
        "    title=\"Meesho Customer Support Chatbot\",\n",
        "    description=\"A chatbot to assist with queries regarding orders, deliveries, and payment issues.\"\n",
        ")\n",
        "\n",
        "# Launch the application\n",
        "if __name__ == \"__main__\":\n",
        "    demo.launch()\n"
      ],
      "metadata": {
        "id": "O_GfVT0lDf_m"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import json\n",
        "import random\n",
        "import nltk\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Dense, Dropout\n",
        "from tensorflow.keras.optimizers import SGD\n",
        "from gtts import gTTS\n",
        "import pickle\n",
        "import gradio as gr\n",
        "nltk.download('punkt_tab')\n",
        "nltk.download('wordnet')\n",
        "\n",
        "# Initialize the lemmatizer\n",
        "lemmatizer = WordNetLemmatizer()\n",
        "\n",
        "# Load intents.json file (sample intents for Meesho chatbot)\n",
        "intents = {\n",
        "    \"intents\": [\n",
        "        {\"tag\": \"greeting\", \"patterns\": [\"Hi\", \"Hello\", \"Hey\"], \"responses\": [\"Hello! How can I assist you today?\"]},\n",
        "        {\"tag\": \"order_status\", \"patterns\": [\"What is the status of my order?\", \"Track my order\"], \"responses\": [\"Please provide your order ID to check the status.\"]},\n",
        "        {\"tag\": \"delivery\", \"patterns\": [\"When will my delivery arrive?\", \"Delivery details\"], \"responses\": [\"Deliveries usually arrive within 3-5 business days.\"]},\n",
        "        {\"tag\": \"payment_issue\", \"patterns\": [\"I have a payment issue\", \"Payment failed\"], \"responses\": [\"I'm sorry to hear that. Please provide more details about the payment issue.\"]},\n",
        "        {\"tag\": \"goodbye\", \"patterns\": [\"Bye\", \"See you later\"], \"responses\": [\"Goodbye! Have a great day!\"]}\n",
        "    ]\n",
        "}\n",
        "\n",
        "# Preprocessing data\n",
        "words = []\n",
        "classes = []\n",
        "documents = []\n",
        "ignore_words = ['?', '!']\n",
        "\n",
        "for intent in intents['intents']:\n",
        "    for pattern in intent['patterns']:\n",
        "        word_list = nltk.word_tokenize(pattern)\n",
        "        words.extend(word_list)\n",
        "        documents.append((word_list, intent['tag']))\n",
        "        if intent['tag'] not in classes:\n",
        "            classes.append(intent['tag'])\n",
        "\n",
        "words = sorted(set([lemmatizer.lemmatize(w.lower()) for w in words if w not in ignore_words]))\n",
        "classes = sorted(set(classes))\n",
        "\n",
        "training = []\n",
        "output_empty = [0] * len(classes)\n",
        "\n",
        "for doc in documents:\n",
        "    bag = []\n",
        "    word_patterns = doc[0]\n",
        "    word_patterns = [lemmatizer.lemmatize(word.lower()) for word in word_patterns]\n",
        "    for word in words:\n",
        "        bag.append(1 if word in word_patterns else 0)\n",
        "\n",
        "    output_row = list(output_empty)\n",
        "    output_row[classes.index(doc[1])] = 1\n",
        "    training.append([bag, output_row])\n",
        "\n",
        "# Convert training data to NumPy arrays with consistent dimensions\n",
        "train_x = np.array([row[0] for row in training])\n",
        "train_y = np.array([row[1] for row in training])\n",
        "\n",
        "# Build the model\n",
        "model = Sequential()\n",
        "model.add(Dense(128, input_shape=(len(train_x[0]),), activation='relu'))\n",
        "model.add(Dropout(0.5))\n",
        "model.add(Dense(64, activation='relu'))\n",
        "model.add(Dropout(0.5))\n",
        "model.add(Dense(len(train_y[0]), activation='softmax'))\n",
        "\n",
        "sgd = SGD(learning_rate=0.01, momentum=0.9, nesterov=True)\n",
        "model.compile(loss='categorical_crossentropy', optimizer=sgd, metrics=['accuracy'])\n",
        "\n",
        "# Train the model\n",
        "model.fit(np.array(train_x), np.array(train_y), epochs=200, batch_size=5, verbose=1)\n",
        "\n",
        "# Save the model and data structures for future use\n",
        "model.save(\"meesho_chatbot_model.h5\")\n",
        "pickle.dump(words, open(\"words.pkl\", \"wb\"))\n",
        "pickle.dump(classes, open(\"classes.pkl\", \"wb\"))\n",
        "\n",
        "# Load trained model and data structures\n",
        "model = tf.keras.models.load_model(\"meesho_chatbot_model.h5\")\n",
        "words = pickle.load(open(\"words.pkl\", \"rb\"))\n",
        "classes = pickle.load(open(\"classes.pkl\", \"rb\"))\n",
        "\n",
        "def clean_up_sentence(sentence):\n",
        "    sentence_words = nltk.word_tokenize(sentence)\n",
        "    sentence_words = [lemmatizer.lemmatize(word.lower()) for word in sentence_words]\n",
        "    return sentence_words\n",
        "\n",
        "def bow(sentence, words):\n",
        "    sentence_words = clean_up_sentence(sentence)\n",
        "    bag = [0] * len(words)\n",
        "    for s in sentence_words:\n",
        "        for i, w in enumerate(words):\n",
        "            if w == s:\n",
        "                bag[i] = 1\n",
        "    return np.array(bag)\n",
        "\n",
        "def predict_class(sentence):\n",
        "    bow_data = bow(sentence, words)\n",
        "    res = model.predict(np.array([bow_data]))[0]\n",
        "    ERROR_THRESHOLD = 0.25\n",
        "    results = [[i, r] for i, r in enumerate(res) if r > ERROR_THRESHOLD]\n",
        "    results.sort(key=lambda x: x[1], reverse=True)\n",
        "    return [{\"intent\": classes[r[0]], \"probability\": str(r[1])} for r in results]\n",
        "\n",
        "def get_response(intents_list):\n",
        "    tag = intents_list[0]['intent']\n",
        "    for intent in intents['intents']:\n",
        "        if intent['tag'] == tag:\n",
        "            return random.choice(intent['responses'])\n",
        "\n",
        "def chatbot_response(user_input):\n",
        "    intents_list = predict_class(user_input)\n",
        "    response_text = get_response(intents_list)\n",
        "    return response_text\n",
        "\n",
        "def generate_audio(text, language=\"en\", gender=\"male\"):\n",
        "    tts_voice_option = gTTS(text=text, lang=language)\n",
        "    audio_file_path = f\"response_{gender}_{language}.mp3\"\n",
        "    tts_voice_option.save(audio_file_path)\n",
        "    return audio_file_path\n",
        "\n",
        "def chatbot_interface(user_input, language=\"en\", gender=\"male\"):\n",
        "    response_text = chatbot_response(user_input)\n",
        "\n",
        "    # Generate audio response based on language and gender preferences\n",
        "    audio_path = generate_audio(response_text, language=language, gender=gender)\n",
        "\n",
        "    return response_text, audio_path\n",
        "\n",
        "# Gradio Interface\n",
        "demo_ui = gr.Interface(\n",
        "    fn=chatbot_interface,\n",
        "    inputs=[\n",
        "        gr.Textbox(lines=2, placeholder=\"Ask me anything about orders or payments...\"),\n",
        "        gr.Radio([\"en\", \"hi\"], label=\"Select Language\"),\n",
        "        gr.Radio([\"male\", \"female\"], label=\"Select Gender\")\n",
        "    ],\n",
        "    outputs=[\"text\", gr.Audio()],\n",
        "    title=\"Meesho Customer Support Chatbot\",\n",
        "    description=\"A chatbot to assist with queries regarding orders, deliveries, and payment issues.\"\n",
        ")\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    demo_ui.launch()\n"
      ],
      "metadata": {
        "id": "AmGC0XZ5GEzK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import requests\n",
        "from bs4 import BeautifulSoup\n",
        "import gradio as gr\n",
        "\n",
        "def get_song_data(query):\n",
        "    # Search for songs related to the query\n",
        "    search_url = f\"https://search.azlyrics.com/search.php?q={query.replace(' ', '+')}\"\n",
        "    try:\n",
        "        response = requests.get(search_url, timeout=10)\n",
        "        response.raise_for_status()  # Raise HTTPError for bad responses (4xx or 5xx)\n",
        "        soup = BeautifulSoup(response.text, \"lxml\")\n",
        "    except requests.exceptions.RequestException as e:\n",
        "        return f\"Error: Could not retrieve search results. {e}\", [], []\n",
        "\n",
        "    # Extract song links and titles\n",
        "    matches = soup.find_all('td', class_='text-left visitedlyr')\n",
        "\n",
        "    if not matches:\n",
        "        return \"No songs found.\", [], []\n",
        "\n",
        "    songs = []\n",
        "    for match in matches:\n",
        "        try:\n",
        "            a_tag = match.find('a')\n",
        "            if a_tag:\n",
        "                title = a_tag.text\n",
        "                link = a_tag['href']\n",
        "                songs.append((title, link))\n",
        "            else:\n",
        "                print(\"Warning: No <a> tag found in the expected <td>.\")\n",
        "                continue\n",
        "        except Exception as e:\n",
        "            print(f\"Error extracting song: {e}\")\n",
        "            continue\n",
        "\n",
        "    if not songs:\n",
        "        return \"No songs found.\", [], []\n",
        "\n",
        "    # Get the most popular song (first in the list)\n",
        "    most_popular_song = songs[0]\n",
        "\n",
        "    # Fetch lyrics for the most popular song\n",
        "    lyrics_url = most_popular_song[1]\n",
        "    try:\n",
        "        lyrics_response = requests.get(lyrics_url, timeout=10)\n",
        "        lyrics_response.raise_for_status()\n",
        "        lyrics_soup = BeautifulSoup(lyrics_response.text, \"lxml\")\n",
        "        lyrics_div = lyrics_soup.find('div', class_='col-xs-12 col-lg-8 text-center')\n",
        "        if lyrics_div:\n",
        "            lyrics = lyrics_div.get_text(separator=\"\\n\").strip()\n",
        "        else:\n",
        "            lyrics = \"Lyrics not found.\"\n",
        "    except requests.exceptions.RequestException as e:\n",
        "        lyrics = f\"Error: Could not retrieve lyrics. {e}\"\n",
        "    except Exception as e:\n",
        "        lyrics = f\"Error processing lyrics: {e}\"\n",
        "\n",
        "    # Return the most popular song, its lyrics, and other related songs\n",
        "    related_songs_list = [song[0] for song in songs[1:]]\n",
        "    return most_popular_song[0], lyrics, related_songs_list\n",
        "\n",
        "def song_finder(input_text):\n",
        "    title, lyrics, related_songs = get_song_data(input_text)\n",
        "\n",
        "    return title, lyrics, related_songs\n",
        "\n",
        "# Create Gradio interface\n",
        "demo = gr.Interface(\n",
        "    fn=song_finder,\n",
        "    inputs=gr.Textbox(lines=2, placeholder=\"Enter a phrase or keyword...\"),\n",
        "    outputs=[\n",
        "        gr.Textbox(label=\"Most Popular Song\"),\n",
        "        gr.Textbox(label=\"Lyrics\", lines=10),\n",
        "        gr.Dataframe(label=\"Related Songs\", headers=[\"Song Titles\"], row_count=\"dynamic\")\n",
        "    ],\n",
        "    title=\"Song Finder\",\n",
        "    description=\"Enter a phrase to find related songs and their lyrics.\"\n",
        ")\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    demo.launch()\n"
      ],
      "metadata": {
        "id": "Ag_gn053Gcew"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import lyricsgenius\n",
        "import gradio as gr\n",
        "import json\n",
        "\n",
        "# Replace with your Genius API access token\n",
        "GENIUS_ACCESS_TOKEN = \"EBb6_tnc9cH7ERPIfx7FC3lT1Tl5M9CDscsUrS6N_9bFDAwRhXG5iLteesTOY1Gh\"\n",
        "\n",
        "genius = lyricsgenius.Genius(GENIUS_ACCESS_TOKEN)\n",
        "genius.verbose = False  # Suppress verbose output\n",
        "genius.remove_section_headers = True  # Remove section headers from lyrics\n",
        "\n",
        "def get_song_data(query):\n",
        "    try:\n",
        "        # Search for the song\n",
        "        songs = genius.search(query, per_page=5)  # Request multiple results\n",
        "\n",
        "        if songs and songs['hits']:  # Ensure results are not empty\n",
        "            first_song = songs['hits'][0]['result']  # Access the first result\n",
        "\n",
        "            song = genius.song(first_song['id'])  # Get song details by ID\n",
        "\n",
        "            if song:\n",
        "                title = song.title\n",
        "                lyrics = song.lyrics\n",
        "                artist = song.artist\n",
        "\n",
        "                return f\"{title} by {artist}\", lyrics, []\n",
        "            else:\n",
        "                return \"Song details not found.\", \"\", []\n",
        "        else:\n",
        "            return \"No songs found.\", \"\", []\n",
        "\n",
        "    except Exception as e:\n",
        "        return f\"Error: {e}\", \"\", []\n",
        "\n",
        "def song_finder(input_text):\n",
        "    title, lyrics, related_songs = get_song_data(input_text)\n",
        "\n",
        "    return title, lyrics, related_songs\n",
        "\n",
        "# Create Gradio interface\n",
        "demo = gr.Interface(\n",
        "    fn=song_finder,\n",
        "    inputs=gr.Textbox(lines=2, placeholder=\"Enter a phrase or keyword...\"),\n",
        "    outputs=[\n",
        "        gr.Textbox(label=\"Most Popular Song\"),\n",
        "        gr.Textbox(label=\"Lyrics\", lines=10),\n",
        "        gr.Dataframe(label=\"Related Songs\", headers=[\"Song Titles\"], row_count=\"dynamic\")\n",
        "    ],\n",
        "    title=\"Song Finder\",\n",
        "    description=\"Enter a phrase to find related songs and their lyrics.\"\n",
        ")\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    demo.launch()\n"
      ],
      "metadata": {
        "id": "8q3Ufz6KMJb4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "hlHex5H9Nxxn"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}