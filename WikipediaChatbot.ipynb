{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "V28",
      "authorship_tag": "ABX9TyNfSmGdzY4qsKKTnAb3jH3h",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "TPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Vicky270506/GenAI/blob/main/WikipediaChatbot.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "578Ki1dl6-c1"
      },
      "outputs": [],
      "source": [
        "from langchain_community.document_loaders import WikipediaLoader\n",
        "docs = WikipediaLoader(query=\"Quantum Physics\", load_max_docs=3).load()"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_community.document_loaders import WikipediaLoader\n",
        "docs = WikipediaLoader(query=\"Quantum Physics\", load_max_docs=3).load()"
      ],
      "metadata": {
        "id": "lj3Lu1Gp9k3S"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_community.vectorstores import FAISS\n",
        "from langchain_huggingface import HuggingFaceEmbeddings\n",
        "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
        "text_splitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=0)\n",
        "\n",
        "# Split the documents into chunks\n",
        "chunks = text_splitter.split_documents(docs)\n",
        "\n",
        "embeddings = HuggingFaceEmbeddings(model_name=\"sentence-transformers/all-mpnet-base-v2\")\n",
        "vector_store = FAISS.from_documents(chunks, embeddings)\n",
        "vector_store.save_local(\"wikipedia_faiss\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6goBd2zs8GZm",
        "outputId": "b4b8c7fe-1cf3-4065-f15e-1fa380380110"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/huggingface_hub/utils/_auth.py:94: UserWarning: \n",
            "The secret `HF_TOKEN` does not exist in your Colab secrets.\n",
            "To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n",
            "You will be able to reuse this secret in all of your notebooks.\n",
            "Please note that authentication is recommended but still optional to access public models or datasets.\n",
            "  warnings.warn(\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def format_response(response):\n",
        "    sources = {doc.metadata['source'] for doc in response[\"source_documents\"]}\n",
        "    return f\"{response['answer']}\\n\\nSources: {', '.join(sources)}\"\n"
      ],
      "metadata": {
        "id": "Xkm5w7-W9y_Z"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "SYSTEM_PROMPT = \"\"\"You are a research assistant.\n",
        "Use formal academic language and always cite sources using [number] notation.\"\"\"\n"
      ],
      "metadata": {
        "id": "0cpPgb0q9_Ta"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain import LLMCheckerChain\n",
        "from langchain_google_genai import GoogleGenerativeAi\n",
        "from google.generativeai import types\n",
        "\n",
        "# Initialize an OpenAI language model but configure it to use Gemini API\n",
        "llm = GoogleGenerativeAi(\n",
        "    temperature=0,\n",
        "    model_name=\"models/gemini-2.0-flash\", # Setting the Gemini model name\n",
        "    google_api_key=\"AIzaSyC2qhbyhfr1DIM4qZ9l_8kwRmxbeuXTKvY\", # Replace with your actual key\n",
        ")\n",
        "checker = LLMCheckerChain.from_llm(llm)\n",
        "answer = \"What is quantum physics?\"\n",
        "verified_answer = checker.run(answer)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 383
        },
        "id": "0_uC_SMb-B0B",
        "outputId": "e9fd5e5e-61c3-4274-894a-4780196c5084"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "ImportError",
          "evalue": "cannot import name 'GoogleGenerativeAi' from 'langchain_google_genai' (/usr/local/lib/python3.11/dist-packages/langchain_google_genai/__init__.py)",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mImportError\u001b[0m                               Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-8-95727cfbc3d1>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mlangchain\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mLLMCheckerChain\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mlangchain_google_genai\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mGoogleGenerativeAi\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mgoogle\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgenerativeai\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mtypes\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;31m# Initialize an OpenAI language model but configure it to use Gemini API\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mImportError\u001b[0m: cannot import name 'GoogleGenerativeAi' from 'langchain_google_genai' (/usr/local/lib/python3.11/dist-packages/langchain_google_genai/__init__.py)",
            "",
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0;32m\nNOTE: If your import is failing due to a missing package, you can\nmanually install dependencies using either !pip or !apt.\n\nTo view examples of installing some common dependencies, click the\n\"Open Examples\" button below.\n\u001b[0;31m---------------------------------------------------------------------------\u001b[0m\n"
          ],
          "errorDetails": {
            "actions": [
              {
                "action": "open_url",
                "actionText": "Open Examples",
                "url": "/notebooks/snippets/importing_libraries.ipynb"
              }
            ]
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_community.document_loaders import WikipediaLoader\n",
        "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
        "from langchain_community.vectorstores import FAISS\n",
        "from langchain.chains import ConversationalRetrievalChain\n",
        "from langchain.memory import ConversationBufferWindowMemory\n",
        "from langchain_huggingface import HuggingFaceEmbeddings\n",
        "from langchain import HuggingFaceHub\n",
        "import gradio as gr\n",
        "import os\n",
        "\n",
        "# Initialize components\n",
        "def initialize_chain():\n",
        "    # 1. Load Wikipedia data\n",
        "    loader = WikipediaLoader(query=\"Machine learning\", load_max_docs=5)\n",
        "    docs = loader.load()\n",
        "\n",
        "    # 2. Split documents\n",
        "    text_splitter = RecursiveCharacterTextSplitter(\n",
        "        chunk_size=1000,\n",
        "        chunk_overlap=200\n",
        "    )\n",
        "    chunks = text_splitter.split_documents(docs)\n",
        "\n",
        "    # 3. Create vector store\n",
        "    embeddings = HuggingFaceEmbeddings(model_name=\"sentence-transformers/all-mpnet-base-v2\")\n",
        "    vector_store = FAISS.from_documents(chunks, embeddings)\n",
        "\n",
        "    # 4. Initialize LLM (Mistral 7B)\n",
        "    huggingfacehub_api_token = os.environ.get(\"HUGGINGFACEHUB_API_TOKEN\", \"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\")\n",
        "    llm = HuggingFaceHub(\n",
        "        repo_id=\"mistralai/Mistral-7B-Instruct-v0.2\",\n",
        "        huggingfacehub_api_token=huggingfacehub_api_token,\n",
        "    )\n",
        "\n",
        "    # 5. Create conversation chain\n",
        "    return ConversationalRetrievalChain.from_llm(\n",
        "        llm=llm,\n",
        "        retriever=vector_store.as_retriever(search_kwargs={\"k\": 3}),  # Increase k to retrieve more documents\n",
        "        memory=ConversationBufferWindowMemory(\n",
        "            memory_key=\"chat_history\",\n",
        "            k=5,\n",
        "            return_messages=True\n",
        "        ),\n",
        "        verbose=True,\n",
        "        # Add chain_type=\"stuff\" for better context handling\n",
        "        chain_type=\"stuff\"\n",
        "    )\n",
        "\n",
        "# Initialize the chain\n",
        "qa_chain = initialize_chain()\n",
        "# Gradio interface\n",
        "def chat_response(message, history):\n",
        "    # Format previous history for LangChain - Updated formatting\n",
        "    formatted_history = []\n",
        "    for human_msg, ai_msg in history:\n",
        "        formatted_history.append((human_msg, ai_msg))  # Use tuples for history entries\n",
        "\n",
        "    # Get response\n",
        "    try:  # Wrap in try-except to handle potential errors\n",
        "        result = qa_chain({\n",
        "            \"question\": message,\n",
        "            \"chat_history\": formatted_history\n",
        "        })\n",
        "    except Exception as e:\n",
        "        return f\"An error occurred: {e}\"  # Return error message to user\n",
        "\n",
        "    # Format response with sources\n",
        "    sources = list(set(doc.metadata['source'] for doc in result[\"source_documents\"]))\n",
        "    response = f\"{result['answer']}\\n\\n**Sources:**\\n\" + \"\\n\".join(sources)\n",
        "\n",
        "    return response\n",
        "\n",
        "# Create Gradio interface\n",
        "with gr.Blocks(theme=gr.themes.Soft()) as demo:\n",
        "    gr.Markdown(\"# Wikipedia Research Assistant\")\n",
        "    chatbot = gr.Chatbot(height=500)\n",
        "    msg = gr.Textbox(label=\"Your Question\")\n",
        "    clear = gr.Button(\"Clear History\")\n",
        "\n",
        "    def user(user_message, history):\n",
        "        return \"\", history + [[user_message, None]]\n",
        "\n",
        "    msg.submit(user, [msg, chatbot], [msg, chatbot], queue=False).then(\n",
        "        chat_response, inputs=[msg, chatbot], outputs=[chatbot]\n",
        "    )\n",
        "    clear.click(lambda: None, None, chatbot, queue=False)\n",
        "\n",
        "# STEP 3: Run the application\n",
        "if __name__ == \"__main__\":\n",
        "    demo.launch(server_port=7865, share=False)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 408
        },
        "id": "uT_RHVAAEUBt",
        "outputId": "1bf92f42-e118-4c66-93ca-ed4bc47ef80f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "ValidationError",
          "evalue": "1 validation error for HuggingFaceHub\ntimeout\n  Extra inputs are not permitted [type=extra_forbidden, input_value=60, input_type=int]\n    For further information visit https://errors.pydantic.dev/2.11/v/extra_forbidden",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mValidationError\u001b[0m                           Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-26-567fff4b1bf0>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     49\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     50\u001b[0m \u001b[0;31m# Initialize the chain\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 51\u001b[0;31m \u001b[0mqa_chain\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0minitialize_chain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     52\u001b[0m \u001b[0;31m# Gradio interface\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     53\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mchat_response\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmessage\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhistory\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-26-567fff4b1bf0>\u001b[0m in \u001b[0;36minitialize_chain\u001b[0;34m()\u001b[0m\n\u001b[1;32m     28\u001b[0m     \u001b[0;31m# 4. Initialize LLM (Mistral 7B)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     29\u001b[0m     \u001b[0mhuggingfacehub_api_token\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0menviron\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"HUGGINGFACEHUB_API_TOKEN\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"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\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 30\u001b[0;31m     llm = HuggingFaceHub(\n\u001b[0m\u001b[1;32m     31\u001b[0m         \u001b[0mrepo_id\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"mistralai/Mistral-7B-Instruct-v0.2\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     32\u001b[0m         \u001b[0mhuggingfacehub_api_token\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mhuggingfacehub_api_token\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/langchain_core/_api/deprecation.py\u001b[0m in \u001b[0;36mwarn_if_direct_instance\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    219\u001b[0m                         \u001b[0mwarned\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    220\u001b[0m                         \u001b[0memit_warning\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 221\u001b[0;31m                     \u001b[0;32mreturn\u001b[0m \u001b[0mwrapped\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    222\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    223\u001b[0m                 obj.__init__ = functools.wraps(obj.__init__)(  # type: ignore[misc]\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/langchain_core/load/serializable.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    128\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__init__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mAny\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mAny\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    129\u001b[0m         \u001b[0;34m\"\"\"\"\"\"\u001b[0m  \u001b[0;31m# noqa: D419\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 130\u001b[0;31m         \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__init__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    131\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    132\u001b[0m     \u001b[0;34m@\u001b[0m\u001b[0mclassmethod\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/pydantic/main.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, **data)\u001b[0m\n\u001b[1;32m    241\u001b[0m         \u001b[0;31m# `__tracebackhide__` tells pytest and some other tools to omit this function from tracebacks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    242\u001b[0m         \u001b[0m__tracebackhide__\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 243\u001b[0;31m         \u001b[0mvalidated_self\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__pydantic_validator__\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalidate_python\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself_instance\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    244\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mvalidated_self\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    245\u001b[0m             warnings.warn(\n",
            "\u001b[0;31mValidationError\u001b[0m: 1 validation error for HuggingFaceHub\ntimeout\n  Extra inputs are not permitted [type=extra_forbidden, input_value=60, input_type=int]\n    For further information visit https://errors.pydantic.dev/2.11/v/extra_forbidden"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_community.document_loaders import WikipediaLoader\n",
        "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
        "from langchain_community.vectorstores import FAISS\n",
        "from langchain.chains import ConversationalRetrievalChain\n",
        "from langchain.memory import ConversationBufferWindowMemory\n",
        "from langchain_huggingface import HuggingFaceEmbeddings\n",
        "from langchain import HuggingFaceHub\n",
        "import gradio as gr\n",
        "import os\n",
        "import threading\n",
        "\n",
        "def launch_gradio():\n",
        "    demo.launch(share=True)\n",
        "\n",
        "threading.Thread(target=launch_gradio).start()\n",
        "\n",
        "# Initialize components\n",
        "def initialize_chain():\n",
        "    # 1. Load Wikipedia data\n",
        "    loader = WikipediaLoader(query=\"Machine learning\", load_max_docs=5)\n",
        "    docs = loader.load()\n",
        "\n",
        "    # 2. Split documents\n",
        "    text_splitter = RecursiveCharacterTextSplitter(\n",
        "        chunk_size=1000,\n",
        "        chunk_overlap=200\n",
        "    )\n",
        "    chunks = text_splitter.split_documents(docs)\n",
        "\n",
        "    # 3. Create vector store\n",
        "    embeddings = HuggingFaceEmbeddings(model_name=\"sentence-transformers/all-mpnet-base-v2\")\n",
        "    vector_store = FAISS.from_documents(chunks[:10], embeddings)\n",
        "    # 4. Initialize LLM (Mistral 7B)\n",
        "    huggingfacehub_api_token = os.environ.get(\"HUGGINGFACEHUB_API_TOKEN\", \"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\")\n",
        "    llm = HuggingFaceHub(\n",
        "        repo_id=\"tiiuae/falcon-7b-instruct\",\n",
        "        huggingfacehub_api_token=huggingfacehub_api_token,\n",
        "        # Remove the 'timeout' parameter\n",
        "        # timeout=60\n",
        "    )\n",
        "\n",
        "    # 5. Create conversation chain\n",
        "    return ConversationalRetrievalChain.from_llm(\n",
        "        llm=llm,\n",
        "        retriever=vector_store.as_retriever(search_kwargs={\"k\": 3}),  # Increase k to retrieve more documents\n",
        "        memory=ConversationBufferWindowMemory(\n",
        "            memory_key=\"chat_history\",\n",
        "            k=5,\n",
        "            return_messages=True\n",
        "        ),\n",
        "        verbose=True,\n",
        "        # Add chain_type=\"stuff\" for better context handling\n",
        "        chain_type=\"stuff\"\n",
        "    )\n",
        "\n",
        "# Initialize the chain\n",
        "qa_chain = initialize_chain()\n",
        "# Gradio interface\n",
        "def chat_response(message, history):\n",
        "    # Format previous history for LangChain - Updated formatting\n",
        "    formatted_history = []\n",
        "    for human_msg, ai_msg in history:\n",
        "        formatted_history.append((human_msg, ai_msg))  # Use tuples for history entries\n",
        "\n",
        "    # Get response\n",
        "    try:  # Wrap in try-except to handle potential errors\n",
        "        result = qa_chain({\n",
        "            \"question\": message,\n",
        "            \"chat_history\": formatted_history\n",
        "        })\n",
        "    except Exception as e:\n",
        "        return f\"An error occurred: {e}\"  # Return error message to user\n",
        "\n",
        "    # Format response with sources\n",
        "    sources = list(set(doc.metadata['source'] for doc in result[\"source_documents\"]))\n",
        "    response = f\"{result['answer']}\\n\\n**Sources:**\\n\" + \"\\n\".join(sources)\n",
        "\n",
        "    return response\n",
        "\n",
        "# Create Gradio interface\n",
        "with gr.Blocks(theme=gr.themes.Soft()) as demo:\n",
        "    gr.Markdown(\"# Wikipedia Research Assistant\")\n",
        "    chatbot = gr.Chatbot(height=500)\n",
        "    msg = gr.Textbox(label=\"Your Question\")\n",
        "    clear = gr.Button(\"Clear History\")\n",
        "\n",
        "    def user(user_message, history):\n",
        "        return \"\", history + [[user_message, None]]\n",
        "\n",
        "    msg.submit(user, [msg, chatbot], [msg, chatbot], queue=False).then(\n",
        "        chat_response, inputs=[msg, chatbot], outputs=[chatbot]\n",
        "    )\n",
        "    clear.click(lambda: None, None, chatbot, queue=False)\n",
        "\n",
        "# STEP 3: Run the application\n",
        "if __name__ == \"__main__\":\n",
        "    demo.launch(server_port=7889, share=True)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 958
        },
        "id": "D2dgN7V3EzS_",
        "outputId": "8bd838b0-4d3c-4fb0-a7d7-a8f0645b26f4"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Exception in thread Thread-9 (launch_gradio):\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/lib/python3.11/threading.py\", line 1045, in _bootstrap_inner\n",
            "    self.run()\n",
            "  File \"/usr/lib/python3.11/threading.py\", line 982, in run\n",
            "    self._target(*self._args, **self._kwargs)\n",
            "  File \"<ipython-input-4-bcd95eb9ef07>\", line 13, in launch_gradio\n",
            "NameError: name 'demo' is not defined\n",
            "/usr/local/lib/python3.11/dist-packages/huggingface_hub/utils/_auth.py:94: UserWarning: \n",
            "The secret `HF_TOKEN` does not exist in your Colab secrets.\n",
            "To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n",
            "You will be able to reuse this secret in all of your notebooks.\n",
            "Please note that authentication is recommended but still optional to access public models or datasets.\n",
            "  warnings.warn(\n",
            "<ipython-input-4-bcd95eb9ef07>:35: LangChainDeprecationWarning: The class `HuggingFaceHub` was deprecated in LangChain 0.0.21 and will be removed in 1.0. An updated version of the class exists in the :class:`~langchain-huggingface package and should be used instead. To use it run `pip install -U :class:`~langchain-huggingface` and import as `from :class:`~langchain_huggingface import HuggingFaceEndpoint``.\n",
            "  llm = HuggingFaceHub(\n",
            "<ipython-input-4-bcd95eb9ef07>:46: LangChainDeprecationWarning: Please see the migration guide at: https://python.langchain.com/docs/versions/migrating_memory/\n",
            "  memory=ConversationBufferWindowMemory(\n",
            "<ipython-input-4-bcd95eb9ef07>:83: UserWarning: You have not specified a value for the `type` parameter. Defaulting to the 'tuples' format for chatbot messages, but this is deprecated and will be removed in a future version of Gradio. Please set type='messages' instead, which uses openai-style dictionaries with 'role' and 'content' keys.\n",
            "  chatbot = gr.Chatbot(height=500)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Colab notebook detected. To show errors in colab notebook, set debug=True in launch()\n",
            "* Running on public URL: https://127826ca1d5cc580c2.gradio.live\n",
            "\n",
            "This share link expires in 72 hours. For free permanent hosting and GPU upgrades, run `gradio deploy` from the terminal in the working directory to deploy to Hugging Face Spaces (https://huggingface.co/spaces)\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "<div><iframe src=\"https://127826ca1d5cc580c2.gradio.live\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import traceback\n",
        "\n",
        "def chat_response(message, history):\n",
        "    print(\"\\n=== New Chat Request ===\")\n",
        "    print(\"User Message:\", message)\n",
        "    print(\"Chat History:\", history)\n",
        "\n",
        "    formatted_history = [(human, ai) for human, ai in history]\n",
        "\n",
        "    try:\n",
        "        result = qa_chain({\n",
        "            \"question\": message,\n",
        "            \"chat_history\": formatted_history\n",
        "        })\n",
        "        print(\"Raw Response:\", result)  # Print the entire response\n",
        "    except Exception as e:\n",
        "        print(\"ERROR:\", traceback.format_exc())  # Print full traceback\n",
        "        return f\"An error occurred: {e}\"\n",
        "\n",
        "    # Extract sources safely\n",
        "    sources = list(set(doc.metadata['source'] for doc in result.get(\"source_documents\", [])))\n",
        "    response = f\"{result.get('answer', 'No response generated.')}\\n\\n**Sources:**\\n\" + \"\\n\".join(sources)\n",
        "\n",
        "    print(\"Final Response:\", response)\n",
        "    return response\n"
      ],
      "metadata": {
        "id": "j192L0hNOuhL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_community.document_loaders import WikipediaLoader\n",
        "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
        "from langchain_community.vectorstores import FAISS\n",
        "from langchain.chains import ConversationalRetrievalChain\n",
        "from langchain.memory import ConversationBufferWindowMemory\n",
        "from langchain_huggingface import HuggingFaceEmbeddings\n",
        "from langchain import HuggingFaceHub\n",
        "import gradio as gr\n",
        "import os\n",
        "import traceback\n",
        "\n",
        "# === Step 1: Initialize LangChain Components ===\n",
        "def initialize_chain():\n",
        "    print(\"Loading Wikipedia data...\")\n",
        "    loader = WikipediaLoader(query=\"Vaaranam Aayiram\", load_max_docs=5)\n",
        "    docs = loader.load()\n",
        "\n",
        "    print(\"Splitting documents...\")\n",
        "    text_splitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=200)\n",
        "    chunks = text_splitter.split_documents(docs)\n",
        "\n",
        "    print(\"Creating vector store...\")\n",
        "    embeddings = HuggingFaceEmbeddings(model_name=\"sentence-transformers/all-mpnet-base-v2\")\n",
        "    vector_store = FAISS.from_documents(chunks, embeddings)\n",
        "\n",
        "    print(\"Initializing LLM...\")\n",
        "    huggingfacehub_api_token = os.getenv(\"HUGGINGFACEHUB_API_TOKEN\", \"your_huggingface_api_token_here\")\n",
        "    llm = HuggingFaceHub(\n",
        "        repo_id=\"mistralai/Mistral-7B-Instruct-v0.2\",\n",
        "        huggingfacehub_api_token=huggingfacehub_api_token\n",
        "    )\n",
        "\n",
        "    print(\"Setting up Conversation Chain...\")\n",
        "    return ConversationalRetrievalChain.from_llm(\n",
        "        llm=llm,\n",
        "        retriever=vector_store.as_retriever(search_kwargs={\"k\": 3}),\n",
        "        memory=ConversationBufferWindowMemory(memory_key=\"chat_history\", k=5, return_messages=True),\n",
        "        verbose=True,\n",
        "        chain_type=\"stuff\"\n",
        "    )\n",
        "\n",
        "# Initialize QA chain\n",
        "qa_chain = initialize_chain()\n",
        "\n",
        "# === Step 2: Define Gradio Chat Function ===\n",
        "def chat_response(message, history):\n",
        "    print(\"\\n=== New Chat Request ===\")\n",
        "    print(\"User Message:\", message)\n",
        "    print(\"Chat History:\", history)\n",
        "\n",
        "    formatted_history = [(human, ai) for human, ai in history]\n",
        "\n",
        "    try:\n",
        "        result = qa_chain({\"question\": message, \"chat_history\": formatted_history})\n",
        "        print(\"Raw Response:\", result)  # Debugging output\n",
        "    except Exception as e:\n",
        "        print(\"ERROR:\", traceback.format_exc())\n",
        "        return history + [[message, f\"An error occurred: {e}\"]]\n",
        "\n",
        "    # Extract sources\n",
        "    sources = list(set(doc.metadata['source'] for doc in result.get(\"source_documents\", [])))\n",
        "    response = f\"{result.get('answer', 'No response generated.')}\\n\\n**Sources:**\\n\" + \"\\n\".join(sources)\n",
        "\n",
        "    print(\"Final Response:\", response)\n",
        "    return history + [[message, response]]\n",
        "\n",
        "# === Step 3: Setup Gradio UI ===\n",
        "with gr.Blocks() as demo:\n",
        "    gr.Markdown(\"# Wikipedia Research Assistant\")\n",
        "    chatbot = gr.Chatbot()\n",
        "    msg = gr.Textbox(label=\"Your Question\")\n",
        "    clear = gr.Button(\"Clear History\")\n",
        "\n",
        "    msg.submit(chat_response, inputs=[msg, chatbot], outputs=[chatbot])\n",
        "    clear.click(lambda: [], None, chatbot, queue=False)  # Reset history on clear\n",
        "\n",
        "# === Step 4: Launch Gradio App ===\n",
        "if __name__ == \"__main__\":\n",
        "    demo.launch(share=True)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 732
        },
        "id": "5M3mFHUMPaPE",
        "outputId": "24100f87-8713-4635-cd14-383f977d88a2"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loading Wikipedia data...\n",
            "Splitting documents...\n",
            "Creating vector store...\n",
            "Initializing LLM...\n",
            "Setting up Conversation Chain...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-19-b71c12ecdb2a>:70: UserWarning: You have not specified a value for the `type` parameter. Defaulting to the 'tuples' format for chatbot messages, but this is deprecated and will be removed in a future version of Gradio. Please set type='messages' instead, which uses openai-style dictionaries with 'role' and 'content' keys.\n",
            "  chatbot = gr.Chatbot()\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Colab notebook detected. To show errors in colab notebook, set debug=True in launch()\n",
            "* Running on public URL: https://11940bc5321045388e.gradio.live\n",
            "\n",
            "This share link expires in 72 hours. For free permanent hosting and GPU upgrades, run `gradio deploy` from the terminal in the working directory to deploy to Hugging Face Spaces (https://huggingface.co/spaces)\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "<div><iframe src=\"https://11940bc5321045388e.gradio.live\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "exidGd61UI_5"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}